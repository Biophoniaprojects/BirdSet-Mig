{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a RESNET model on the esc50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/GADME\n"
     ]
    }
   ],
   "source": [
    "# change working directory to the root of the project\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision\n",
    "import torch\n",
    "import torchmetrics.classification\n",
    "\n",
    "from src.modules.base_module import BaseModule\n",
    "from src.datamodule.esc50_datamodule import ESC50\n",
    "from src.utils.ast_extractor import CustomASTFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_config = OmegaConf.create({\n",
    "    \"dataset\": {\n",
    "        \"data_dir\": \"/workspaces/GADME/data\",\n",
    "        \"dataset_name\": \"esc50\",\n",
    "        \"hf_path\": \"ashraq/esc50\",\n",
    "        \"hf_name\": None,\n",
    "        \"seed\": 42,\n",
    "        \"feature_extractor\": {\n",
    "            \"_target_\": 'src.utils.ast_extractor.CustomASTFeatureExtractor',\n",
    "            \"n_classes\": 50,\n",
    "            \"n_workers\": 1,\n",
    "            \"column_list\": [\"input_values\", \"target\"],\n",
    "            \"val_split\": 0.2,\n",
    "            \"sampling_rate\": 32000,\n",
    "            \"return_tensor\": 'pt',\n",
    "        },\n",
    "        \"n_classes\": 50,\n",
    "        \"n_workers\": 16,\n",
    "        \"column_list\": [\"input_values\", \"target\"],\n",
    "        \"val_split\": 0.2,\n",
    "    },\n",
    "    \"loaders\": {\n",
    "        \"train\": {\n",
    "            \"batch_size\": 6,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": 4,\n",
    "            \"drop_last\": False,\n",
    "            \"pin_memory\": False\n",
    "        },\n",
    "        \"valid\": {\n",
    "            \"batch_size\": 6,\n",
    "            \"shuffle\": False,\n",
    "            \"num_workers\": 4,\n",
    "            \"drop_last\": False,\n",
    "            \"pin_memory\": False\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"batch_size\": 6,\n",
    "            \"shuffle\": False,\n",
    "            \"num_workers\": 4,\n",
    "            \"drop_last\": False,\n",
    "            \"pin_memory\": False\n",
    "        },\n",
    "    },\n",
    "    \"transforms\": {\n",
    "        \"use_channel_dim\": False,\n",
    "        \"normalize\": True,\n",
    "        \"use_spectrogram\": True,\n",
    "        \"n_fft\": 1024,\n",
    "        \"hop_length\": 79,\n",
    "        \"n_mels\": 128,\n",
    "        \"db_scale\": True,\n",
    "        \"target_height\": 32,\n",
    "        \"target_width\": 32,\n",
    "        \"waveform_augmentations\": {\n",
    "            \"colored_noise\": {\n",
    "                \"prob\": 0.5,\n",
    "                \"min_snr_in_db\": 3.0,\n",
    "                \"max_snr_in_db\": 30.0,\n",
    "                \"min_f_decay\": -2.0,\n",
    "                \"max_f_decay\": 2.0\n",
    "            },\n",
    "        },\n",
    "        \"spectrogram_augmentations\": {\n",
    "            \"time_masking\": {\n",
    "                \"time_mask_param\": 100,\n",
    "                \"prob\": 0.5\n",
    "            },\n",
    "            \"frequency_masking\": {\n",
    "                \"freq_mask_param\": 100,\n",
    "                \"prob\": 0.5\n",
    "            },\n",
    "            \"time_stretch\": {\n",
    "                \"prob\": 0.5,\n",
    "                \"min_rate\": 0.8,\n",
    "                \"max_rate\": 1.2\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_datamodule = ESC50(dataset=esc50_config.dataset, loaders=esc50_config.loaders, transforms=esc50_config.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5896a7405d704a169bbfe057a7da9334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f58165fc1e141e0a5c6b1d8b3a4e7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7600671051594cff985ae41f4cf4e044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f6a0e737f0442aabffbdabc89b16e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "esc50_datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_datamodule.setup(stage=\"fit\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = esc50_datamodule.train_dataloader()\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[[-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541],\n",
       "          [-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541],\n",
       "          [-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541],\n",
       "          ...,\n",
       "          [-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541],\n",
       "          [-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541],\n",
       "          [-4.8541, -4.8541, -4.8541,  ..., -4.8541, -4.8541, -4.8541]],\n",
       " \n",
       "         [[-3.7393, -3.7393, -3.7393,  ..., -1.9202, -1.2638, -1.1575],\n",
       "          [-3.7393, -3.7393, -3.7393,  ..., -1.3351, -0.6787, -0.5724],\n",
       "          [-3.7393, -3.7393, -3.7393,  ..., -1.6349, -0.7000,  0.0219],\n",
       "          ...,\n",
       "          [-3.7393, -3.7393, -3.7393,  ...,  2.0272,  1.7800,  1.4278],\n",
       "          [-3.7393, -3.7393, -3.7393,  ...,  2.0349,  1.8543,  1.5428],\n",
       "          [-3.7393, -3.7393, -3.7393,  ...,  1.7031,  1.6676,  1.5917]],\n",
       " \n",
       "         [[-1.7175, -1.8153, -1.9695,  ..., -2.4231, -2.1222, -2.0084],\n",
       "          [-1.1324, -1.2302, -1.3844,  ..., -1.8380, -1.5372, -1.4233],\n",
       "          [-0.2913, -0.1431, -0.0734,  ..., -0.4350, -0.4419, -0.4869],\n",
       "          ...,\n",
       "          [-1.8977, -1.7469, -1.6637,  ..., -2.8011, -2.6433, -2.5593],\n",
       "          [-2.5146, -2.5530, -2.4703,  ..., -2.9573, -3.1118, -2.9100],\n",
       "          [-2.2116, -2.3027, -2.4598,  ..., -2.9792, -3.3178, -3.3277]],\n",
       " \n",
       "         [[ 2.8698,  2.8489,  2.7771,  ...,  2.4980,  2.3351,  2.1826],\n",
       "          [ 3.4549,  3.4339,  3.3622,  ...,  3.0831,  2.9202,  2.7677],\n",
       "          [ 1.8896,  2.1151,  2.4154,  ...,  2.8558,  2.7002,  2.4982],\n",
       "          ...,\n",
       "          [ 4.0705,  4.0855,  4.1007,  ...,  3.1424,  2.9968,  2.9096],\n",
       "          [ 3.8120,  3.9302,  4.1247,  ...,  4.7740,  4.7034,  4.6207],\n",
       "          [ 3.9849,  3.9966,  4.0249,  ...,  5.1646,  5.1005,  5.0132]],\n",
       " \n",
       "         [[-1.4576, -1.3250, -1.2594,  ..., -0.5344, -0.7705, -0.9568],\n",
       "          [-0.8725, -0.7399, -0.6743,  ...,  0.0507, -0.1854, -0.3717],\n",
       "          [ 0.7622,  0.7349,  0.6601,  ...,  1.0007,  0.8288,  0.6748],\n",
       "          ...,\n",
       "          [ 1.5016,  1.6155,  1.6927,  ...,  1.3337,  1.4600,  1.5065],\n",
       "          [ 1.4304,  1.2824,  1.2563,  ...,  1.4989,  1.5336,  1.5014],\n",
       "          [ 1.3160,  0.9703,  0.5337,  ...,  1.1024,  1.1270,  1.0808]],\n",
       " \n",
       "         [[-1.1962, -1.1791, -1.1705,  ...,  0.9503,  0.9748,  0.9309],\n",
       "          [-0.6111, -0.5941, -0.5854,  ...,  1.5354,  1.5598,  1.5160],\n",
       "          [ 0.8072,  0.8598,  0.9632,  ...,  1.6059,  1.7555,  1.8473],\n",
       "          ...,\n",
       "          [ 1.1655,  1.1749,  1.1856,  ...,  0.8621,  1.0697,  1.1902],\n",
       "          [ 0.3138,  0.2787,  0.2418,  ...,  0.8601,  0.9695,  1.1994],\n",
       "          [ 0.7331,  0.7693,  0.8914,  ...,  1.5627,  1.5186,  1.5868]]]),\n",
       " 'labels': tensor([13,  3, 15, 42, 10,  8])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4e88690d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the first spectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(batch['input_values'][0].squeeze().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_config = OmegaConf.create({\n",
    "    \"network\": {\n",
    "        \"model_name\": \"resnet50\",\n",
    "        \"model\": {\n",
    "            \"_target_\": 'torchvision.models.resnet50',\n",
    "            \"weights\": 'ResNet50_Weights.IMAGENET1K_V1'\n",
    "        },\n",
    "        \"torch_compile\": False\n",
    "    },\n",
    "    \"output_activation\": {\n",
    "        \"_target_\": 'torch.softmax',\n",
    "        \"dim\": 1\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"_target_\": 'torch.nn.CrossEntropyLoss'\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"_target_\": 'torch.optim.Adam',\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"_target_\": 'torch.optim.lr_scheduler',\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"main\": {\n",
    "            \"_target_\": 'torchmetrics.classification.Accuracy',\n",
    "            \"task\": \"multiclass\",\n",
    "            \"num_classes\": 50,\n",
    "            \"top_k\": 1,\n",
    "        },\n",
    "        \"val_best\": {   \n",
    "            \"_target_\": 'torchmetrics.MaxMetric',\n",
    "        }\n",
    "    },\n",
    "    \"num_epochs\": 10,\n",
    "    \"logging_params\": None,\n",
    "    \"len_trainset\": 50000\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnetModule = BaseModule(\n",
    "    network=module_config.network,\n",
    "    output_activation= module_config.output_activation,\n",
    "    loss=module_config.loss,\n",
    "    optimizer=module_config.optimizer,\n",
    "    lr_scheduler=module_config.lr_scheduler,\n",
    "    metrics=module_config.metrics,\n",
    "    logging_params=module_config.logging_params,\n",
    "    num_epochs=module_config.num_epochs,\n",
    "    len_trainset=module_config.len_trainset\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModule(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (train_metric): MulticlassAccuracy()\n",
       "  (train_add_metrics): MetricCollection,\n",
       "    postfix=/train\n",
       "  )\n",
       "  (valid_metric): MulticlassAccuracy()\n",
       "  (valid_metric_best): MaxMetric()\n",
       "  (valid_add_metrics): MetricCollection,\n",
       "    postfix=/valid\n",
       "  )\n",
       "  (test_metric): MulticlassAccuracy()\n",
       "  (test_add_metrics): MetricCollection,\n",
       "    postfix=/test\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnetModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.models.benchmark_models import LightningResNet, ResNetVersion\n",
    "resnetModule = LightningResNet(\n",
    "    baseline_architecture='resnet18',\n",
    "    num_classes=50,\n",
    "    num_channels=1,\n",
    "    learning_rate=0.001,\n",
    "    pretrained=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss   | 0     \n",
      "1 | acc       | MulticlassAccuracy | 0     \n",
      "2 | model     | ResNet             | 11.2 M\n",
      "-------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.777    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7bf6d14dcc42bdb1bbea98008a240a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (str, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/GADME/notebooks/9-rs-train-resnet.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f64617461312f7273636877696e6765722f6769742f4741444d45222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f64617461312f7273636877696e6765722f6769742f4741444d452f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Bkiel.ins.informatik.uni-kiel.de/workspaces/GADME/notebooks/9-rs-train-resnet.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f64617461312f7273636877696e6765722f6769742f4741444d45222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f64617461312f7273636877696e6765722f6769742f4741444d452f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Bkiel.ins.informatik.uni-kiel.de/workspaces/GADME/notebooks/9-rs-train-resnet.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f64617461312f7273636877696e6765722f6769742f4741444d45222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f64617461312f7273636877696e6765722f6769742f4741444d452f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Bkiel.ins.informatik.uni-kiel.de/workspaces/GADME/notebooks/9-rs-train-resnet.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(resnetModule, esc50_datamodule)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1064\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspaces/GADME/src/modules/models/benchmark_models.py:151\u001b[0m, in \u001b[0;36mLightningResNet.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Defines a single validation step.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    batch (Tuple[torch.Tensor, torch.Tensor]): A tuple containing input data and corresponding labels.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m    batch_idx (int): The index of the current batch.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m--> 151\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\n\u001b[1;32m    153\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(preds, y)\n\u001b[1;32m    154\u001b[0m acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macc(preds, y)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspaces/GADME/src/modules/models/benchmark_models.py:100\u001b[0m, in \u001b[0;36mLightningResNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Defines the forward pass, i.e., the computation performed on each call.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m        torch.Tensor: The output tensor produced by the ResNet model.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gadme-zu58s5te-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (str, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "trainer = Trainer(max_epochs=10, devices=1)\n",
    "trainer.fit(resnetModule, esc50_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gadme-zu58s5te-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
