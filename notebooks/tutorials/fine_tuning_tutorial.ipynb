{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BirdSet Fine-Tuning Tutorial\n",
    "\n",
    "This notebook details how to fine-tune models on BirdSet data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions are just helper functions for the code below. These can be ignored for now and looked up when they are used in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Lightning Module\n",
    "\n",
    "This snippet defines a custom LightningModule. It is needed as a wrapper for models loaded from Hugging Face if you want to train using the Lightning trainer, as it expects an instance of `Lightning.LightningModule` and models loaded from Hugging Face are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTForAudioClassification, ConvNextForImageClassification\n",
    "import lightning as l\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "    \n",
    "class ConvNextClassifierLightningModule(l.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes,\n",
    "            num_epochs,\n",
    "            ):\n",
    "        super(ConvNextClassifierLightningModule, self).__init__()\n",
    "        self.model = ConvNextForImageClassification.from_pretrained(\n",
    "            \"DBD-research-group/ConvNeXT-Base-BirdSet-XCM\",\n",
    "            # since HSN has a different number of labels we need to specify that\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_epochs = num_epochs\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.model(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        values = batch['input_values']\n",
    "        labels = batch['labels']\n",
    "        logits = self(values)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/values.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def smart_sampling(dataset, label_name, class_limit, event_limit):\n",
    "        def _unique_identifier(x, labelname):\n",
    "            file = x[\"filepath\"]\n",
    "            label = x[labelname]\n",
    "            return {\"id\": f\"{file}-{label}\"}\n",
    "\n",
    "        class_limit = class_limit if class_limit else -float(\"inf\")\n",
    "        dataset = dataset.map(\n",
    "            lambda x: _unique_identifier(x, label_name), desc=\"sampling: unique-identifier\"\n",
    "        )\n",
    "        df = pd.DataFrame(dataset)\n",
    "        path_label_count = df.groupby([\"id\", label_name], as_index=False).size()\n",
    "        path_label_count = path_label_count.set_index(\"id\")\n",
    "        class_sizes = df.groupby(label_name).size()\n",
    "\n",
    "        for label in tqdm(class_sizes.index, desc=\"sampling\"):\n",
    "            current = path_label_count[path_label_count[label_name] == label]\n",
    "            total = current[\"size\"].sum()\n",
    "            most = current[\"size\"].max()\n",
    "\n",
    "            while total > class_limit or most != event_limit:\n",
    "                largest_count = current[\"size\"].value_counts()[current[\"size\"].max()]\n",
    "                n_largest = current.nlargest(largest_count + 1, \"size\")\n",
    "                to_del = n_largest[\"size\"].max() - n_largest[\"size\"].min()\n",
    "\n",
    "                idxs = n_largest[n_largest[\"size\"] == n_largest[\"size\"].max()].index\n",
    "                if (\n",
    "                    total - (to_del * largest_count) < class_limit\n",
    "                    or most == event_limit\n",
    "                    or most == 1\n",
    "                ):\n",
    "                    break\n",
    "                for idx in idxs:\n",
    "                    current.at[idx, \"size\"] = current.at[idx, \"size\"] - to_del\n",
    "                    path_label_count.at[idx, \"size\"] = (\n",
    "                        path_label_count.at[idx, \"size\"] - to_del\n",
    "                    )\n",
    "\n",
    "                total = current[\"size\"].sum()\n",
    "                most = current[\"size\"].max()\n",
    "\n",
    "        event_counts = Counter(dataset[\"id\"])\n",
    "\n",
    "        all_file_indices = {label: [] for label in event_counts.keys()}\n",
    "        for idx, label in enumerate(dataset[\"id\"]):\n",
    "            all_file_indices[label].append(idx)\n",
    "\n",
    "        limited_indices = []\n",
    "        for file, indices in all_file_indices.items():\n",
    "            limit = path_label_count.loc[file][\"size\"]\n",
    "            limited_indices.extend(random.sample(indices, limit))\n",
    "\n",
    "        dataset = dataset.remove_columns(\"id\")\n",
    "        return dataset.select(limited_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def classes_one_hot(batch, num_classes):\n",
    "        \"\"\"\n",
    "        Converts class labels to one-hot encoding.\n",
    "\n",
    "        This method takes a batch of data and converts the class labels to one-hot encoding.\n",
    "        The one-hot encoding is a binary matrix representation of the class labels.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A batch of data. The batch should be a dictionary where the keys are the field names and the values are the field data.\n",
    "\n",
    "        Returns:\n",
    "            dict: The batch with the \"labels\" field converted to one-hot encoding. The keys are the field names and the values are the field data.\n",
    "        \"\"\"\n",
    "        label_list = [y for y in batch[\"labels\"]]\n",
    "        class_one_hot_matrix = torch.zeros(\n",
    "            (len(label_list), num_classes), dtype=torch.float\n",
    "        )\n",
    "\n",
    "        for class_idx, idx in enumerate(label_list):\n",
    "            class_one_hot_matrix[class_idx, idx] = 1\n",
    "\n",
    "        class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
    "        return {\"labels\": class_one_hot_matrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Datamodule\n",
    "\n",
    "When using a lightning trainer you can either pass a Datamodule or the Dataloaders needed for the planed task. Typically a Datamodule would load the data, preprocess it and apply transforms. In this case the datamodule will only hold the data and provide Dataloaders. The other steps are done separately so as to not abstract each step too much.  \n",
    "For a more typical implementation of Datamodules you should look into the `BirdSetDataModule` in `birdset/datamodules/birdset_datamodule.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDatamodule(L.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage):\n",
    "        pass \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.dataset[\"train\"], batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.dataset[\"valid\"], batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.dataset[\"test\"], batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transforms Wrapper\n",
    "\n",
    "This class will help in handling the application of transforms on the data as well as converting the waveforms to spectrogramms. Transforms are stored in a list and then applied in the list order when a call to this class is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchaudio.transforms import Spectrogram, MelScale\n",
    "from birdset.datamodule.components.resize import Resizer\n",
    "from birdset.datamodule.components.augmentations import PowerToDB\n",
    "import numpy as np\n",
    "import torch_audiomentations\n",
    "import torchvision.transforms\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    spectrogram_conversion: Spectrogram | None = Spectrogram(\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            power=2.0,\n",
    "    )\n",
    "    resizer: Resizer | None = Resizer(\n",
    "            db_scale=True,\n",
    "    ),\n",
    "    melscale_conversion: MelScale | None = MelScale(\n",
    "            n_mels=128,\n",
    "            sample_rate=32000,\n",
    "            n_stft=513, # n_fft//2+1\n",
    "    ),\n",
    "    dbscale_conversion: PowerToDB | None = PowerToDB(),\n",
    "    normalize_spectrogram: bool = True,\n",
    "    mean: float = -4.268,\n",
    "    std: float = -4.569,\n",
    "\n",
    "class BasicTransformsWrapper():\n",
    "    def __init__(self,\n",
    "                 wav_transforms,\n",
    "                 spec_transforms,\n",
    "                 decoding,\n",
    "                 feature_extractor,\n",
    "                 nocall_sampler,\n",
    "                 processing_config = ProcessingConfig):\n",
    "        self.wav_transforms = torch_audiomentations.Compose(\n",
    "            transforms=wav_transforms,\n",
    "            output_type=\"object_dict\")\n",
    "        self.spec_transforms = torchvision.transforms.Compose(\n",
    "            transforms=spec_transforms\n",
    "        )\n",
    "        self.processing = processing_config\n",
    "        self.nocall_sampler = nocall_sampler\n",
    "        self.decoding = decoding\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __call__(self, batch, **kwds):\n",
    "\n",
    "        batch = self.decoding(batch)\n",
    "\n",
    "        waveform_batch = self._get_waveform_batch(batch)\n",
    "\n",
    "        attention_mask = waveform_batch[\"attention_mask\"]\n",
    "        input_values = waveform_batch[\"input_values\"]\n",
    "        input_values = input_values.unsqueeze(1)\n",
    "        labels = torch.tensor(batch[\"labels\"])\n",
    "\n",
    "        input_values, labels = self._waveform_augmentation(input_values, labels)\n",
    "\n",
    "        print(input_values)\n",
    "\n",
    "\n",
    "        if self.nocall_sampler:\n",
    "            #input_values, labels = self.nocall_sampler(input_values, labels)\n",
    "            pass\n",
    "\n",
    "        if self.processing.spectrogram_conversion is not None:\n",
    "            spectrograms = self.processing.spectrogram_conversion(input_values)\n",
    "\n",
    "            if self.spec_transforms:\n",
    "                spectrograms = self.spec_transforms(spectrograms)\n",
    "\n",
    "            if self.processing.melscale_conversion:\n",
    "                spectrograms = self.processing.melscale_conversion(spectrograms)\n",
    "\n",
    "            if self.processing.dbscale_conversion:\n",
    "                spectrograms = self.processing.dbscale_conversion(spectrograms)\n",
    "\n",
    "            if self.processing.resizer:\n",
    "                spectrograms = self.processing.resizer.resize_spectrogram_batch(spectrograms)\n",
    "                \n",
    "            if self.processing.normalize_spectrogram:\n",
    "                spectrograms = (spectrograms - self.processing.mean) / self.processing.std\n",
    "            \n",
    "            input_values = spectrograms\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float16)\n",
    "\n",
    "        print(input_values, labels)\n",
    "        return input_values, labels\n",
    "\n",
    "    def _get_waveform_batch(self, batch):\n",
    "        waveform_batch = [audio[\"array\"] for audio in batch[\"audio\"]]\n",
    "        \n",
    "        # extract/pad/truncate\n",
    "        # max_length determains the difference with input waveforms as factor 5 (embedding)\n",
    "        max_length = int(int(self.feature_extractor.sampling_rate) * int(self.decoding.max_len))\n",
    "        waveform_batch = self.feature_extractor(\n",
    "            waveform_batch,\n",
    "            padding='max_length',\n",
    "            max_length=max_length, \n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        return waveform_batch\n",
    "\n",
    "    def _waveform_augmentation(self, input_values, labels):\n",
    "        labels = labels.unsqueeze(1).unsqueeze(1)\n",
    "        output_dict = self.wav_transforms(\n",
    "            samples=input_values, \n",
    "            sample_rate=self.feature_extractor.sampling_rate,\n",
    "            targets=labels\n",
    "        )\n",
    "        labels = output_dict.targets.squeeze(1).squeeze(1)\n",
    "        \n",
    "        return input_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "There are two ways to use BirdSet datasets:\n",
    "\n",
    "1. using BirdSets inbuild datamodules which manage the loading, preparing and transformation/augmentation of data\n",
    "2. loading, preparing and transforming/augmenting the data manually\n",
    "\n",
    "Both of these options are shown in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BirdSet Datamodules\n",
    "\n",
    "This section shows how to load a dataset with BirdSets datamodules. These modules handle everythig for us and have a great amount of customizability. For more information about what the parameters of the various configs do, a look into the `birdset-pipeline_tutorial` notebook is advised. The configuration of datamodules is explained there.\n",
    "\n",
    "Please note that `PretrainDataModule` should be used for the `XCL` and `XCM` datasets as they only have `train` splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "from birdset.configs.datamodule_configs import DatasetConfig, LoadersConfig\n",
    "from birdset.datamodule.components.transforms import BirdSetTransformsWrapper\n",
    "from birdset.datamodule.components import XCEventMapping\n",
    "\n",
    "dataset_config = DatasetConfig(\n",
    "        data_dir='../../data_birdset/HSN',\n",
    "        hf_path='DBD-research-group/BirdSet',\n",
    "        hf_name='HSN',\n",
    "        n_workers=3,\n",
    "        val_split=0.2,\n",
    "        task=\"multilabel\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sampling_rate=32000,\n",
    "        seed=2\n",
    "    )\n",
    "loaders_config = LoadersConfig()\n",
    "transforms_config = BirdSetTransformsWrapper()\n",
    "mapper_config = XCEventMapping()\n",
    "\n",
    "datamodule = BirdSetDataModule(\n",
    "    dataset = dataset_config,\n",
    "    loaders=loaders_config,\n",
    "    transforms=transforms_config,\n",
    "    mapper=mapper_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above only configures the datamodule. To actully use it we need to also prepare the data (which also downloads it) and setup the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495534b921ec47d1bafe5a88876b73a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling: unique-identifier:   0%|          | 0/38170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling: 100%|██████████| 21/21 [00:01<00:00, 14.90it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96caa2490c14ebbbe0f1e3b3f74e0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot-encoding train labels. (num_proc=3):   0%|          | 0/17940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c895ed1efc046988481093deeca3259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot-encoding test_5s labels. (num_proc=3):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/home/rantjuschin/BirdSet/birdset/datamodule/base_datamodule.py:478: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3d64ccc3da4a46b8627c2b4c50ad7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7179b342a3404cb0809d9ecfc77175b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ea8273bde042e3ba4dfd0626cf0482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Manually\n",
    "\n",
    "If you want to implement your own data pipeline you can also load the datasets through Hugging Face. This section will detail that aproach.  \n",
    "Some amount of BirdSet Code will still be used here so as to not fill up this notebook with helper functions. If you want to, you can look up how those specific methods are coded and copy that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"DBD-research-group/BirdSet\",\n",
    "    name=\"HSN\",\n",
    "    cache_dir=\"../../data_birdset/HSN\",\n",
    "    num_proc=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 5460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 10296\n",
      "    })\n",
      "    test_5s: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "})\n",
      "{'bytes': None, 'path': '/home/rantjuschin/BirdSet/data_birdset/HSN/downloads/extracted/e8d87da059276237fe3a09f216dfe50ac2186afa53bf9c3b9e14c93979cbeba4/XC805337.ogg'}\n",
      "[[2.112, 6.416], [9.68, 11.328], [10.592, 11.776], [12.528, 13.632], [15.136, 17.888], [18.32, 19.456], [19.936, 21.12], [21.584, 22.8], [23.248, 24.816], [25.2, 26.112]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][0][\"audio\"])\n",
    "print(dataset[\"train\"][0][\"detected_events\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the Data\n",
    "\n",
    "You may heave seen that the `audio` column doesn't actually contain any audio data. The loaded dataset as it is, only contains the filepaths of the respective audiosamples. As shown above those files may also contain multiple birdcall events per file which is why it is not encouraged to directly load the whole audio into the sample (it may also just be too big). For BirdSet we typically specify some kind of eventlimit per file and per class and extract and map those events to singular samples in the dataset.\n",
    "\n",
    "**Please Note:** Only the `train` split needs to be processed this way. Both the `test_5s` and `test` splits do not need to be processed like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 38170\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 10296\n",
      "    })\n",
      "    test_5s: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "})\n",
      "/home/rantjuschin/BirdSet/data_birdset/HSN/downloads/extracted/e8d87da059276237fe3a09f216dfe50ac2186afa53bf9c3b9e14c93979cbeba4/XC805337.ogg\n",
      "[12.528, 13.632]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "from birdset.datamodule.components.event_mapping import XCEventMapping\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].cast_column(\n",
    "    column=\"audio\",\n",
    "    feature=Audio(\n",
    "        sampling_rate=32_000,\n",
    "        mono=True,\n",
    "        decode=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mapper = XCEventMapping()\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    mapper,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=300,\n",
    "    num_proc=3,\n",
    "    desc=\"Train event mapping\"\n",
    ")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(\"audio\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0][\"filepath\"])\n",
    "print(dataset[\"train\"][0][\"detected_events\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every event in the train split has been extracted into it's own sample. If you want to, you can limit these to some amount of events per class or per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling: 100%|██████████| 21/21 [00:01<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 17940\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 10296\n",
      "    })\n",
      "    test_5s: Dataset({\n",
      "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"] = smart_sampling(\n",
    "    dataset=dataset[\"train\"],\n",
    "    label_name=\"ebird_code\",\n",
    "    class_limit=500,\n",
    "    event_limit=5\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of this part is to remove unnecessary columns or even whole splits. For BirdSet we use the `test_5s` split for multilabel testing. It contains 5 second long soundscape samples that may contain multiple birds per sample.  \n",
    "To use it you need to remove the `test` split and rename the `test_5s` split to `test`.\n",
    "\n",
    "Additionaly the dataset contains multiple columns that are not used and as such can be removed to make it more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
       "        num_rows: 17940\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_multilabel', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\"train\": dataset[\"train\"], \"test\": dataset[\"test_5s\"]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'labels', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
       "        num_rows: 17940\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'labels', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'detected_events', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"ebird_code_multilabel\", \"labels\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio', 'low_freq', 'high_freq', 'ebird_code', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel'] \n",
      " ['low_freq', 'high_freq', 'ebird_code', 'ebird_code_secondary', 'call_type', 'sex', 'lat', 'long', 'length', 'microphone', 'license', 'source', 'local_time', 'event_cluster', 'peaks', 'quality', 'recordist', 'genus', 'species_group', 'order', 'genus_multilabel', 'species_group_multilabel', 'order_multilabel']\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = {\"filepath\", \"labels\", \"detected_events\", \"start_time\", \"end_time\"}\n",
    "\n",
    "removable_train_columns = [column for column in dataset[\"train\"].column_names if column not in columns_to_keep]\n",
    "removable_test_columns = [column for column in dataset[\"test\"].column_names if column not in columns_to_keep]\n",
    "\n",
    "print(removable_test_columns,\"\\n\", removable_train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(removable_train_columns)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(removable_test_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
      "        num_rows: 17940\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78733288eb0648edbfb54996e6b21248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot-encoding labels. (num_proc=4):   0%|          | 0/17940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1340802/375418494.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/tmp/ipykernel_1340802/375418494.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/tmp/ipykernel_1340802/375418494.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n",
      "/tmp/ipykernel_1340802/375418494.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_one_hot_matrix = torch.tensor(class_one_hot_matrix, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "                    lambda batch: classes_one_hot(batch, num_classes=21),\n",
    "                    batched=True,\n",
    "                    batch_size=300,\n",
    "                    load_from_cache_file=True,\n",
    "                    num_proc=4,\n",
    "                    desc=f\"One-hot-encoding labels.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data\n",
    "\n",
    "The train split should be split into train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
      "        num_rows: 14352\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
      "        num_rows: 3588\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "dataset = DatasetDict({\"train\": splits[\"train\"], \"valid\": splits[\"test\"], \"test\": dataset[\"test\"]})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Transforms\n",
    "\n",
    "As the datasets can get very big very quickly we suggest applying transforms on-the-fly. That means that transforms are only applied to a batch if it is requested (e.g. by a trainer). With huggingface datasets that is possible through the `set_transform` method. For this you will need a wrapper class that will be able to handle multiple transforms on the data. Here only a very basic one is implemented but like before you can look up a more sophisticated one in the BirdSet code (`BirdSetTransformsWrapper` in `birdset/datamodule/components/transforms.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoding & Extracting\n",
    "\n",
    "As previously mentioned the loaded dataset currently only contains filepaths but to apply transforms or use the data otherwise actual audio data is needed. As the name implies BirdSets `EventDecoding` class takes an event and pulls the associated audio data out of the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.datamodule.components.event_decoding import EventDecoding\n",
    "from birdset.datamodule.components.feature_extraction import DefaultFeatureExtractor\n",
    "\n",
    "decoder = EventDecoding(\n",
    "    min_len=1,\n",
    "    max_len=5,\n",
    "    sampling_rate=32000,\n",
    "    extension_time=8,\n",
    "    extracted_interval=5\n",
    ")\n",
    "\n",
    "feature_extractor = DefaultFeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=32000,\n",
    "    padding_value=0.0,\n",
    "    return_attention_mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nocall Sampling\n",
    "\n",
    "Nocall Sampling uses other soundfiles to introduce samples which don't have any birdcalls in them as such samples are not included by default. To use Nocall Sampling you need to download background noise. BirdSet provides such functionality under `resources/utils/download_background_noise.py`. You will also need this background noise to use `AddBackgroundNoise` in the Waveform Transforms section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.datamodule.components.augmentations import NoCallMixer\n",
    "\n",
    "nocall = NoCallMixer(\n",
    "    directory=\"../../data_birdset/dcase18\",\n",
    "    p=0.075,\n",
    "    sampling_rate=32000,\n",
    "    length=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Waveform Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_transforms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = MultilabelMix(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from birdset.datamodule.components.augmentations import MultilabelMix\n",
    "\n",
    "multilabel_mix = MultilabelMix(\n",
    "    p=0.7,\n",
    "    min_snr_in_db=3.0,\n",
    "    max_snr_in_db=30.0,\n",
    "    mix_target=\"union\"\n",
    ")\n",
    "\n",
    "wav_transforms.append(multilabel_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = AddBackgroundNoise(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from birdset.datamodule.components.augmentations import AddBackgroundNoise\n",
    "\n",
    "background_noise = AddBackgroundNoise(\n",
    "    p=0.5,\n",
    "    min_snr_in_db=3,\n",
    "    max_snr_in_db=30,\n",
    "    sample_rate=32000,\n",
    "    target_rate=32000,\n",
    "    background_paths=\"../../data_birdset/background_noise\"\n",
    ")\n",
    "\n",
    "wav_transforms.append(background_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch_audiomentations import AddColoredNoise\n",
    "\n",
    "colored_noise = AddColoredNoise(\n",
    "    p=0.2,\n",
    "    max_f_decay=2,\n",
    "    min_f_decay=-2,\n",
    "    max_snr_in_db=30,\n",
    "    min_snr_in_db=3\n",
    ")\n",
    "\n",
    "wav_transforms.append(colored_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = Gain(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch_audiomentations import Gain\n",
    "\n",
    "gain = Gain(\n",
    "    p=0.2,\n",
    "    min_gain_in_db=-18,\n",
    "    max_gain_in_db=6\n",
    ")\n",
    "\n",
    "wav_transforms.append(gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectrogramm Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "spectrogramm_conversion = Spectrogram(\n",
    "    n_fft=1024,\n",
    "    hop_length=320,\n",
    "    power=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MelScale\n",
    "\n",
    "melscale_conversion = MelScale(\n",
    "    n_mels=128,\n",
    "    sample_rate=32000,\n",
    "    n_stft=513\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.datamodule.components.augmentations import PowerToDB\n",
    "\n",
    "dbscale_conversion = PowerToDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.datamodule.components.resize import Resizer\n",
    "\n",
    "resizer = Resizer(\n",
    "    db_scale=True,\n",
    "    target_height=None,\n",
    "    target_width=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_config = ProcessingConfig(\n",
    "    spectrogram_conversion=spectrogramm_conversion,\n",
    "    resizer=resizer,\n",
    "    melscale_conversion=melscale_conversion,\n",
    "    dbscale_conversion=dbscale_conversion,\n",
    "    normalize_spectrogram=True,\n",
    "    mean=-4.268,\n",
    "    std=4.569\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectrogram Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_transforms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomApply\n",
    "from torchaudio.transforms import FrequencyMasking\n",
    "\n",
    "frequency_masking = RandomApply(\n",
    "    p=0.5,\n",
    "    transforms=[\n",
    "        FrequencyMasking(\n",
    "            freq_mask_param=100,\n",
    "            iid_masks=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "spec_transforms.append(frequency_masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomApply\n",
    "from torchaudio.transforms import TimeMasking\n",
    "\n",
    "time_masking = RandomApply(\n",
    "    p=0.3,\n",
    "    transforms=[\n",
    "        TimeMasking(\n",
    "            time_mask_param=100,\n",
    "            iid_masks=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "spec_transforms.append(time_masking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On The Fly Transforming\n",
    "\n",
    "As mentioned we want to apply the transforms on a batch only as it is getting processed. To achieve that behaviour we use the `set_transform` mathod of huggingface datatets. The `test` split doesn't get transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = BasicTransformsWrapper(\n",
    "    wav_transforms=wav_transforms,\n",
    "    spec_transforms=spec_transforms,\n",
    "    decoding=decoder,\n",
    "    feature_extractor=feature_extractor,\n",
    "    processing_config=processing_config,\n",
    "    nocall_sampler=nocall\n",
    ")\n",
    "\n",
    "dataset[\"train\"].set_transform(transforms, output_all_columns=False)\n",
    "dataset[\"valid\"].set_transform(transforms, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping the Data in a Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CustomDatamodule(dataset=dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
       "        num_rows: 14352\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
       "        num_rows: 3588\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'labels', 'detected_events'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.5296e-02, -5.6836e-02, -4.9056e-02,  ...,  4.2388e-03,\n",
      "          -6.7417e-04, -3.1544e-05]],\n",
      "\n",
      "        [[-2.2003e-06, -1.9877e-06, -2.6309e-05,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 4.0981e-03,  5.0559e-03,  1.2545e-04,  ..., -6.4322e-03,\n",
      "          -1.3973e-03,  2.2024e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3061e-04,  2.0128e-03,  2.3983e-03,  ...,  1.8485e-03,\n",
      "           3.9585e-04, -1.4355e-03]],\n",
      "\n",
      "        [[-8.0808e-03, -6.6102e-03, -6.1482e-03,  ..., -2.4820e-02,\n",
      "          -2.5045e-02, -2.5033e-02]],\n",
      "\n",
      "        [[ 2.5449e-02,  1.6932e-02,  6.3593e-03,  ..., -3.8995e-03,\n",
      "          -4.1311e-03, -4.2805e-03]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1340802/3817574989.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dl \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/arrow_dataset.py:2815\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[0;32m-> 2815\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "dl = datamodule.train_dataloader()\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BirdSets Pretrained Models with BirdSet\n",
    "\n",
    "Here a model pretrained on the `XCL` dataset is loaded. Since it was trained on `XCL` and we want to use it on `HSN` we need to fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for DBD-research-group/BirdSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/DBD-research-group/BirdSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at DBD-research-group/ConvNeXT-Base-BirdSet-XCL and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9736]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9736, 1024]) in the checkpoint and torch.Size([21, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultilabelModule(\n",
       "  (loss): BCEWithLogitsLoss()\n",
       "  (model): ConvNextClassifier(\n",
       "    (model): ConvNextForImageClassification(\n",
       "      (convnext): ConvNextModel(\n",
       "        (embeddings): ConvNextEmbeddings(\n",
       "          (patch_embeddings): Conv2d(1, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (layernorm): ConvNextLayerNorm()\n",
       "        )\n",
       "        (encoder): ConvNextEncoder(\n",
       "          (stages): ModuleList(\n",
       "            (0): ConvNextStage(\n",
       "              (downsampling_layer): Identity()\n",
       "              (layers): Sequential(\n",
       "                (0): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (1): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (2): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ConvNextStage(\n",
       "              (downsampling_layer): Sequential(\n",
       "                (0): ConvNextLayerNorm()\n",
       "                (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (layers): Sequential(\n",
       "                (0): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (1): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (2): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ConvNextStage(\n",
       "              (downsampling_layer): Sequential(\n",
       "                (0): ConvNextLayerNorm()\n",
       "                (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (layers): Sequential(\n",
       "                (0): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (1): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (2): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (3): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (4): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (5): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (6): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (7): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (8): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (9): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (10): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (11): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (12): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (13): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (14): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (15): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (16): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (17): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (18): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (19): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (20): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (21): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (22): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (23): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (24): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (25): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (26): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): ConvNextStage(\n",
       "              (downsampling_layer): Sequential(\n",
       "                (0): ConvNextLayerNorm()\n",
       "                (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (layers): Sequential(\n",
       "                (0): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (1): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (2): ConvNextLayer(\n",
       "                  (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                  (layernorm): ConvNextLayerNorm()\n",
       "                  (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELUActivation()\n",
       "                  (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (classifier): Linear(in_features=1024, out_features=21, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_metric): cmAP()\n",
       "  (valid_metric): cmAP()\n",
       "  (test_metric): cmAP()\n",
       "  (valid_metric_best): MaxMetric()\n",
       "  (valid_add_metrics): MetricCollection(\n",
       "    (MultilabelAUROC): MultilabelAUROC()\n",
       "    (T1Accuracy): TopKAccuracy()\n",
       "    (T3Accuracy): TopKAccuracy()\n",
       "    (mAP): mAP(),\n",
       "    prefix=val/\n",
       "  )\n",
       "  (test_add_metrics): MetricCollection(\n",
       "    (MultilabelAUROC): MultilabelAUROC()\n",
       "    (T1Accuracy): TopKAccuracy()\n",
       "    (T3Accuracy): TopKAccuracy()\n",
       "    (mAP): mAP(),\n",
       "    prefix=test/\n",
       "  )\n",
       "  (test_complete_metrics): MetricCollection(\n",
       "    (cmAP5): cmAP5(\n",
       "      (multilabel_ap): MultilabelAveragePrecision()\n",
       "    )\n",
       "    (pcmAP): pcmAP(),\n",
       "    prefix=test/\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from birdset.modules.models.convnext import ConvNextClassifier\n",
    "from birdset.modules.multilabel_module import MultilabelModule\n",
    "from birdset.configs import NetworkConfig, LRSchedulerConfig, MultilabelMetricsConfig, LoggingParamsConfig\n",
    "\n",
    "network = NetworkConfig(\n",
    "    model=ConvNextClassifier(\n",
    "        checkpoint=\"DBD-research-group/ConvNeXT-Base-BirdSet-XCM\",\n",
    "        num_classes=datamodule.num_classes,\n",
    "        num_channels=1,\n",
    "    ),\n",
    "    model_name=\"convnext\",\n",
    "    model_type=\"vision\"\n",
    ")\n",
    "\n",
    "model = MultilabelModule(\n",
    "    network=network,\n",
    "    num_epochs=5,\n",
    "    len_trainset=datamodule.len_trainset,\n",
    "    task=datamodule.task,\n",
    "    batch_size=datamodule.train_batch_size\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models through Hugging Face\n",
    "\n",
    "Loading a model through Hugging Face and then training it using a Lightning trainer is a bit more work as the trainer expects a `LightningModule`. This means that we need to wrap the model in such a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7079052f894142a83e71bf4bd40c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/17.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55a281856ca4349a361705382b588c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at DBD-research-group/ConvNeXT-Base-BirdSet-XCM and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([411]) in the checkpoint and torch.Size([22]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([411, 1024]) in the checkpoint and torch.Size([22, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNextClassifierLightningModule(\n",
       "  (model): ConvNextForImageClassification(\n",
       "    (convnext): ConvNextModel(\n",
       "      (embeddings): ConvNextEmbeddings(\n",
       "        (patch_embeddings): Conv2d(1, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (layernorm): ConvNextLayerNorm()\n",
       "      )\n",
       "      (encoder): ConvNextEncoder(\n",
       "        (stages): ModuleList(\n",
       "          (0): ConvNextStage(\n",
       "            (downsampling_layer): Identity()\n",
       "            (layers): Sequential(\n",
       "              (0): ConvNextLayer(\n",
       "                (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (1): ConvNextLayer(\n",
       "                (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (2): ConvNextLayer(\n",
       "                (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNextStage(\n",
       "            (downsampling_layer): Sequential(\n",
       "              (0): ConvNextLayerNorm()\n",
       "              (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            )\n",
       "            (layers): Sequential(\n",
       "              (0): ConvNextLayer(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (1): ConvNextLayer(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (2): ConvNextLayer(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNextStage(\n",
       "            (downsampling_layer): Sequential(\n",
       "              (0): ConvNextLayerNorm()\n",
       "              (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "            )\n",
       "            (layers): Sequential(\n",
       "              (0): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (1): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (2): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (3): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (4): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (5): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (6): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (7): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (8): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (9): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (10): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (11): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (12): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (13): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (14): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (15): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (16): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (17): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (18): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (19): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (20): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (21): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (22): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (23): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (24): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (25): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (26): ConvNextLayer(\n",
       "                (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNextStage(\n",
       "            (downsampling_layer): Sequential(\n",
       "              (0): ConvNextLayerNorm()\n",
       "              (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "            )\n",
       "            (layers): Sequential(\n",
       "              (0): ConvNextLayer(\n",
       "                (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (1): ConvNextLayer(\n",
       "                (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (2): ConvNextLayer(\n",
       "                (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "                (layernorm): ConvNextLayerNorm()\n",
       "                (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELUActivation()\n",
       "                (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (classifier): Linear(in_features=1024, out_features=22, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASTLightningModule is a custom class defined in the \"Helper\" section\n",
    "# HSN contains 21 classes +1 for nocall\n",
    "model = ConvNextClassifierLightningModule(22, num_epochs=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Fine-Tuning with a Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=model.num_epochs,\n",
    "    gradient_clip_val=0.5,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 11.06 MiB is free. Process 1074382 has 9.54 GiB memory in use. Process 1214932 has 712.00 MiB memory in use. Including non-PyTorch memory, this process has 326.00 MiB memory in use. Of the allocated memory 168.53 MiB is allocated by PyTorch, and 3.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:957\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:154\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# let the precision plugin convert the module here so that this strategy hook can decide the order\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# of operations\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mconvert_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/strategies/single_device.py:79\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model must be set before self.model.to()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:55\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     54\u001b[0m _update_properties(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 11.06 MiB is free. Process 1074382 has 9.54 GiB memory in use. Process 1214932 has 712.00 MiB memory in use. Including non-PyTorch memory, this process has 326.00 MiB memory in use. Of the allocated memory 168.53 MiB is allocated by PyTorch, and 3.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.fit(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/rantjuschin/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95f78d5876b403f8e87aa370b0e2eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rantjuschin/BirdSet/birdset/datamodule/components/transforms.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 19.06 MiB is free. Process 388427 has 232.00 MiB memory in use. Process 388495 has 258.00 MiB memory in use. Process 3792592 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 9.82 GiB memory in use. Of the allocated memory 9.61 GiB is allocated by PyTorch, and 9.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:788\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    787\u001b[0m )\n\u001b[0;32m--> 788\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    790\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:424\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BirdSet/birdset/modules/multilabel_module.py:62\u001b[0m, in \u001b[0;36mMultilabelModule.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 62\u001b[0m     test_loss, preds, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m#save targets and predictions for test_epoch_end\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_targets\u001b[38;5;241m.\u001b[39mappend(targets\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/BirdSet/birdset/modules/base_module.py:157\u001b[0m, in \u001b[0;36mBaseModule.model_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 157\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mask \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_info\u001b[38;5;241m.\u001b[39mvalid_test_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/BirdSet/birdset/modules/base_module.py:129\u001b[0m, in \u001b[0;36mBaseModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BirdSet/birdset/modules/models/convnext.py:113\u001b[0m, in \u001b[0;36mConvNextClassifier.forward\u001b[0;34m(self, input_values, labels)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m, input_values: torch\u001b[38;5;241m.\u001b[39mTensor, labels: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Defines the forward pass of the ConvNext model.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m        torch.Tensor: The output of the ConvNext model.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:431\u001b[0m, in \u001b[0;36mConvNextForImageClassification.forward\u001b[0;34m(self, pixel_values, labels, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 431\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    435\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:364\u001b[0m, in \u001b[0;36mConvNextModel.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 364\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    367\u001b[0m     embedding_output,\n\u001b[1;32m    368\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    369\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:148\u001b[0m, in \u001b[0;36mConvNextEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(embeddings)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/birdset/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 19.06 MiB is free. Process 388427 has 232.00 MiB memory in use. Process 388495 has 258.00 MiB memory in use. Process 3792592 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 9.82 GiB memory in use. Of the allocated memory 9.61 GiB is allocated by PyTorch, and 9.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.test(datamodule=datamodule, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
