{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Download dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"ppeyret/NBMSet24\",\n",
    "    name=\"NBMSet24\",\n",
    "    cache_dir=\"D:/NBMSet24\",\n",
    "    num_proc=None, # put and integer (number of workers) here if you want to use multiprecessing\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path=\"D:/NBMSet24\"\n",
    "bg_noise_datapath=\"D:/Birdset/background_noise\"\n",
    "# bg_noise_datapath=r\"D:\\MNHN_no_bird_call\\Annotations_Yves_pourPaulPeyret\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD TRANSFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from birdset.datamodule.components.transforms import (\n",
    "    BirdSetTransformsWrapper,\n",
    "    PreprocessingConfig,\n",
    ")\n",
    "from custom_event_decoding import CustomEventDecoding\n",
    "from birdset.datamodule.components.event_decoding import EventDecoding\n",
    "from birdset.datamodule.components.feature_extraction import DefaultFeatureExtractor\n",
    "from birdset.datamodule.components.augmentations import (\n",
    "    NoCallMixer,\n",
    "    MultilabelMix,\n",
    "    AddBackgroundNoise,\n",
    "    PowerToDB,\n",
    ")\n",
    "from birdset.datamodule.components.resize import Resizer\n",
    "from torch_audiomentations import AddColoredNoise, Gain\n",
    "from torchaudio.transforms import Spectrogram, MelScale, FrequencyMasking, TimeMasking\n",
    "from torchvision.transforms import RandomApply\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"EVENT DECODING:\n",
    "Loads audio from files, extracting a segment based on event timestamps or manually provided start/end times.\n",
    "Ensures minimum and maximum segment length, adjusting as necessary.\n",
    "Performs time extension when events are too short, centering them in an extended window.\n",
    "Extracts a fixed-length random segment from extended events.\n",
    "Resamples audio to a standardized sampling rate.\"\"\"\n",
    "decoder = CustomEventDecoding(\n",
    "    min_len=1, max_len=5, sampling_rate=32000, extension_time=8, extracted_interval=5\n",
    ")\n",
    "\n",
    "feature_extractor = DefaultFeatureExtractor(\n",
    "    feature_size=1, sampling_rate=32000, padding_value=0.0, return_attention_mask=False\n",
    ")\n",
    "\n",
    "nocall = NoCallMixer(\n",
    "    directory=bg_noise_datapath,\n",
    "    p=0.075,\n",
    "    sampling_rate=32000,\n",
    "    length=5,\n",
    ")\n",
    "\n",
    "wav_transforms = {\n",
    "    # \"multilabel_mix\": MultilabelMix(\n",
    "    #     p=0.7, min_snr_in_db=3.0, max_snr_in_db=30.0, mix_target=\"union\"\n",
    "    # ),\n",
    "    # \"add_background_noise\": AddBackgroundNoise(\n",
    "    #     p=0.5,\n",
    "    #     min_snr_in_db=3,\n",
    "    #     max_snr_in_db=30,\n",
    "    #     sample_rate=32000,\n",
    "    #     target_rate=32000,\n",
    "    #     background_paths=bg_noise_datapath,\n",
    "    # ),\n",
    "    # \"add_colored_noise\": AddColoredNoise(\n",
    "    #     p=0.2, max_f_decay=2, min_f_decay=-2, max_snr_in_db=30, min_snr_in_db=3\n",
    "    # ),\n",
    "    \"gain\": Gain(p=0.2, min_gain_in_db=-18, max_gain_in_db=6),\n",
    "}\n",
    "\n",
    "preprocessing = PreprocessingConfig(\n",
    "    spectrogram_conversion=Spectrogram(n_fft=1024, hop_length=320, power=2.0),\n",
    "    resizer=Resizer(db_scale=True, target_height=None, target_width=None),\n",
    "    melscale_conversion=MelScale(n_mels=128, sample_rate=32000, n_stft=513),\n",
    "    dbscale_conversion=PowerToDB(),\n",
    "    normalize_spectrogram=True,\n",
    "    mean=-4.268,\n",
    "    std=4.569,\n",
    ")\n",
    "\n",
    "spec_transforms = {\n",
    "#     \"frequency_masking\": RandomApply(\n",
    "#         p=0.5, transforms=[FrequencyMasking(freq_mask_param=100, iid_masks=True)]\n",
    "#     ),\n",
    "#     \"time_masking\": RandomApply(\n",
    "#         p=0.5, transforms=[TimeMasking(time_mask_param=100, iid_masks=True)]\n",
    "#     ),\n",
    "}\n",
    "\n",
    "\n",
    "birdset_transforms = BirdSetTransformsWrapper(\n",
    "    task=\"multilabel\",\n",
    "    sampling_rate=32000,\n",
    "    model_type=\"vision\",\n",
    "    max_length=5,\n",
    "    decoding=decoder,\n",
    "    feature_extractor=feature_extractor,\n",
    "    # nocall_sampler=nocall,\n",
    "    waveform_augmentations=wav_transforms,\n",
    "    preprocessing=preprocessing,\n",
    "    spectrogram_augmentations=spec_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter labels with a list of species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# Define the target species list (replace with actual species codes)\n",
    "# target_species = [\"Turdus iliacus\", \"Turdus philomelos\", \"Turdus merula\"]\n",
    "target_species=[\n",
    "\"Alauda arvensis\",\n",
    "\"Motacilla alba\",\n",
    "\"Motacilla flava\",\n",
    "\"Branta bernicla\",\n",
    "\"Nycticorax nycticorax\",\n",
    "\"Calidris alpina\",\n",
    "\"Gallinago gallinago\",\n",
    "\"Coturnix coturnix\",\n",
    "\"Anas platyrhynchos\",\n",
    "\"Carduelis carduelis\",\n",
    "\"Tringa nebularia\",\n",
    "\"Tringa ochropus\",\n",
    "\"Actitis hypoleucos\",\n",
    "\"Strix aluco\",\n",
    "\"Corvus corone\",\n",
    "\"Numenius arquata\",\n",
    "\"Numenius phaeopus\",\n",
    "\"Gallinula chloropus\",\n",
    "\"Charadrius hiaticula\",\n",
    "\"Turdus iliacus\",\n",
    "\"Turdus philomelos\",\n",
    "\"Haematopus ostralegus\",\n",
    "\"Ardea cinerea\",\n",
    "\"Melanitta nigra\",\n",
    "\"Turdus merula\",\n",
    "\"Passer domesticus\",\n",
    "\"Charadrius dubius\",\n",
    "\"Fringilla coelebs\",\n",
    "\"Anthus pratensis\",\n",
    "\"Erithacus rubecula\",\n",
    "]\n",
    "\n",
    "# Create a ClassLabel feature for target species\n",
    "target_class_label = datasets.ClassLabel(names=target_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to assign the new class label\n",
    "def assign_target_label(example):\n",
    "    # Check if ebird_code is in target species\n",
    "    if example[\"label\"] in target_species:\n",
    "        example[\"target_label\"] = target_class_label.str2int(example[\"label\"])\n",
    "        example[\"ebird_code\"] = target_class_label.str2int(example[\"label\"])\n",
    "        example[\"ebird_code_multilabel\"] = [target_class_label.str2int(example[\"label\"])]\n",
    "    else:\n",
    "        # Assign None (-1) for non-target species\n",
    "        example[\"target_label\"] = -1\n",
    "        example[\"ebird_code\"] = -1\n",
    "        example[\"ebird_code_multilabel\"] = [-1]\n",
    "    return example\n",
    "\n",
    "# Apply transformation\n",
    "dataset = dataset.map(assign_target_label)\n",
    "\n",
    "# Set target_label as a ClassLabel feature, ignoring non-target values\n",
    "dataset = dataset.filter(lambda x: x[\"target_label\"] != -1).cast_column(\"target_label\", target_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of labels you want to keep\n",
    "# from datasets import DatasetDict\n",
    "# target_labels = {\"Turdus iliacus\", \"Turdus philomelos\", \"Turdus merula\"}  # Use a set for faster lookup\n",
    "\n",
    "# # Filter train and test splits\n",
    "# filtered_train = dataset[\"train\"].filter(lambda example: example[\"label\"] in target_labels)\n",
    "# filtered_test = dataset[\"test\"].filter(lambda example: example[\"label\"] in target_labels)\n",
    "\n",
    "# # Replace the original dataset with filtered subsets\n",
    "# filtered_dataset = DatasetDict()\n",
    "# filtered_dataset[\"train\"] = filtered_train\n",
    "# filtered_dataset[\"test\"] = filtered_test\n",
    "\n",
    "# # Verify the filtering\n",
    "# print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_keep = {\"filepath\", \"labels\", \"start_time\", \"end_time\"}\n",
    "\n",
    "# removable_train_columns = [\n",
    "#     column for column in dataset[\"train\"].column_names if column not in columns_to_keep\n",
    "# ]\n",
    "# removable_test_columns = [\n",
    "#     column for column in dataset[\"test\"].column_names if column not in columns_to_keep\n",
    "# ]\n",
    "# print(removable_test_columns, \"\\n\", removable_train_columns)\n",
    "# # %%\n",
    "# dataset[\"train\"] = dataset[\"train\"].remove_columns(removable_train_columns)\n",
    "# dataset[\"test\"] = dataset[\"test\"].remove_columns(removable_test_columns)\n",
    "# # %%\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels_train = set(dataset[\"train\"][\"label\"])\n",
    "unique_labels_test = set(dataset[\"test\"][\"label\"])\n",
    "\n",
    "# Get all unique labels across both splits\n",
    "all_unique_labels = unique_labels_train.union(unique_labels_test)\n",
    "\n",
    "# Print the unique labels\n",
    "print(\"Unique labels in train set:\", unique_labels_train)\n",
    "print(\"Unique labels in test set:\", unique_labels_test)\n",
    "print(\"All unique labels:\", all_unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"target_label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one hot\n",
    "from toolkit import classes_one_hot\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: classes_one_hot(batch, num_classes=len(target_species)),\n",
    "    batched=True,\n",
    "    batch_size=300,\n",
    "    load_from_cache_file=True,\n",
    "    num_proc=1,\n",
    "    desc=f\"One-hot-encoding labels.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_table=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_table['train'][0]['filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].set_transform(birdset_transforms, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a test dataset already sliced you may want remove eventmapping from test transorm\n",
    "test_transforms = BirdSetTransformsWrapper(\n",
    "    task=\"multilabel\",\n",
    "    sampling_rate=32000,\n",
    "    model_type=\"vision\",\n",
    "    max_length=5,\n",
    "    decoding=decoder,\n",
    "    feature_extractor=feature_extractor,\n",
    "    nocall_sampler=None,\n",
    "    waveform_augmentations=[],\n",
    "    preprocessing=preprocessing,\n",
    "    spectrogram_augmentations=[],\n",
    ")\n",
    "\n",
    "dataset[\"test\"].set_transform(test_transforms, output_all_columns=False)# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"valid\"]=dataset[\"test\"] # TODO: create a real test split for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from toolkit import CustomDatamodule\n",
    "# wrapping up in datamodule\n",
    "datamodule = CustomDatamodule(\n",
    "    dataset=dataset, batch_size=32, num_workers=0, num_classes=len(target_species), task=\"multilabel\"\n",
    ")\n",
    "\n",
    "datamodule.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sample = next(iter(dl))[\"input_values\"][0]\n",
    "\n",
    "plt.imshow(np.flipud(sample.squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPORT ALL TENSORS TO PNG\n",
    "# from toolkit import save_dataloader_tensors_as_png\n",
    "\n",
    "# save_dataloader_tensors_as_png(dl,output_dir=\"tensor_plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH=10\n",
    "num_classes=len(target_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_module import ConvNextClassifierLightningModule\n",
    "\n",
    "model = ConvNextClassifierLightningModule(num_classes=num_classes, num_epochs=N_EPOCH)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import RichModelSummary\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name=\"first_model\", tracking_uri=\"mlruns/\"\n",
    ")\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=\"./callback_checkpoints\",\n",
    "    monitor=\"val/BCEWithLogitsLoss\",\n",
    "    verbose=False,\n",
    "    save_last=False,\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    auto_insert_metric_name=False,\n",
    "    save_weights_only=False,\n",
    "    every_n_train_steps=None,\n",
    "    train_time_interval=None,\n",
    "    every_n_epochs=1,\n",
    "    save_on_train_epoch_end=None,\n",
    ")\n",
    "\n",
    "rich_model_summary = RichModelSummary(max_depth=1)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=N_EPOCH,\n",
    "    gradient_clip_val=0.5,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=1,\n",
    "    callbacks=[model_checkpoint, rich_model_summary],\n",
    "    logger=mlflow_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(datamodule=datamodule, model=model, ckpt_path=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset-mig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
