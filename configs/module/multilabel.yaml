_target_: src.modules.base_module.BaseModule

defaults:
  - _self_
  - network: ast.yaml

task: multilabel

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-5
  weight_decay: 0.01

loss:
  _target_: torch.nn.BCEWithLogitsLoss
  
lr_scheduler:
  scheduler:
    _target_: transformers.get_linear_schedule_with_warmup
  extras:
    interval: step
    warmup_ratio: 0.05

metrics: 
  main: 
    _target_: "torchmetrics.classification.AveragePrecision"
    task: "multilabel"
    num_labels: ${datamodule.dataset.n_classes}
    average: macro
    thresholds: null
  val_best: 
    _target_: "torchmetrics.MaxMetric"
  additional: 
    f1:
      _target_: "torchmetrics.classification.F1Score"
      num_labels: ${datamodule.dataset.n_classes}  
      average: macro
      task: "multilabel"
    auroc:
      _target_: "torchmetrics.classification.AUROC"
      task: "multilabel"
      num_labels: ${datamodule.dataset.n_classes} 
      average: "macro"
      thresholds: null
    cmap:
      _target_: src.modules.metrics.cMAP
      num_labels: ${datamodule.dataset.n_classes}
      sample_threshold: 0
    cmap5:
      _target_: src.modules.metrics.cMAP5
      num_labels: ${datamodule.dataset.n_classes}
      padding_factor: 5

output_activation: 
  _target_: "torch.sigmoid"

logging_params:
  on_step: False
  on_epoch: True
  sync_dist: False
  prog_bar: True  

